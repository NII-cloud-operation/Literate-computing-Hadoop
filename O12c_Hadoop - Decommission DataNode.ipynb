{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dafa2-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8db18c-495a-11e8-8391-0242ac130002",
     "previous": null
    }
   },
   "source": [
    "# Hadoop - Decommission DataNode\n",
    "\n",
    "---\n",
    "\n",
    "Slave NodeのHDDの故障時などに、Slave NodeをDecommissionするための手順です。\n",
    "\n",
    "我々のオペレーションでは、DataNodeのDecommission後、マシンを再起動するため、YARNのNodeManager, HBaseのRegionServerもあわせて停止しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8db18c-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8db786-495a-11e8-8391-0242ac130002",
     "previous": "9c8dafa2-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "## *Operation Note*\n",
    "\n",
    "*This is a cell for your own recording. ここに経緯を記述*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8db786-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8db9de-495a-11e8-8391-0242ac130002",
     "previous": "9c8db18c-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "# Notebookと対象(Hadoopクラスタ)のBinding\n",
    "\n",
    "Inventory中のgroup名でBind対象を指示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "lc_cell_meme": {
     "current": "9c8db9de-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dbd58-495a-11e8-8391-0242ac130002",
     "previous": "9c8db786-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [],
   "source": [
    "target_group = 'hadoop_all_testcluster'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dbd58-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dbf4c-495a-11e8-8391-0242ac130002",
     "previous": "9c8db9de-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "処理を実行するホストの情報を指示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dbf4c-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dc136-495a-11e8-8391-0242ac130002",
     "previous": "9c8dbd58-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [],
   "source": [
    "# 状態を確認する処理を行うNameNode\n",
    "client_host = 'hadoop_namenode_primary'\n",
    "\n",
    "# デコミッションする対象のDataNode\n",
    "target_host = \"XXX.XXX.XXX.114\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dc136-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dc32a-495a-11e8-8391-0242ac130002",
     "previous": "9c8dbf4c-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "# DataNodeのデコミッション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dc32a-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dc514-495a-11e8-8391-0242ac130002",
     "previous": "9c8dc136-495a-11e8-8391-0242ac130002"
    },
    "sub-structure": {
     "name": {
      "section": "デコミッションしたいノードの情報を確認する"
     },
     "symphonic": {
      "id": "313d1f05-cf47-415e-8255-6f62ca23cc46"
     }
    }
   },
   "source": [
    "## デコミッションしたいノードの情報を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dc514-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dc6d6-495a-11e8-8391-0242ac130002",
     "previous": "9c8dc32a-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "デコミッションしたいノードのNameを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dc6d6-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dc8c0-495a-11e8-8391-0242ac130002",
     "previous": "9c8dc514-495a-11e8-8391-0242ac130002"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mXXX.XXX.XXX.70 | SUCCESS | rc=0 >>\r\n",
      "Configured Capacity: 422216597504 (393.22 GB)\r\n",
      "Present Capacity: 389017650397 (362.30 GB)\r\n",
      "DFS Remaining: 388825679069 (362.12 GB)\r\n",
      "DFS Used: 191971328 (183.08 MB)\r\n",
      "DFS Used%: 0.05%\r\n",
      "Under replicated blocks: 10\r\n",
      "Blocks with corrupt replicas: 0\r\n",
      "Missing blocks: 0\r\n",
      "Missing blocks (with replication factor 1): 0\r\n",
      "\r\n",
      "-------------------------------------------------\r\n",
      "Live datanodes (4):\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.114:50010 (testvm007)\r\n",
      "Hostname: testvm007\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 3747840 (3.57 MB)\r\n",
      "Non DFS Used: 8198860551 (7.64 GB)\r\n",
      "DFS Remaining: 97351540985 (90.67 GB)\r\n",
      "DFS Used%: 0.00%\r\n",
      "DFS Remaining%: 92.23%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 7\r\n",
      "Last contact: Mon Aug 22 11:12:55 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.73:50010 (testvm004)\r\n",
      "Hostname: testvm004\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 62914560 (60 MB)\r\n",
      "Non DFS Used: 8333143732 (7.76 GB)\r\n",
      "DFS Remaining: 97158091084 (90.49 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 92.05%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 9\r\n",
      "Last contact: Mon Aug 22 11:12:53 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.112:50010 (testvm005)\r\n",
      "Hostname: testvm005\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 62648320 (59.75 MB)\r\n",
      "Non DFS Used: 8468127329 (7.89 GB)\r\n",
      "DFS Remaining: 97023373727 (90.36 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 91.92%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 11\r\n",
      "Last contact: Mon Aug 22 11:12:54 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.113:50010 (testvm006)\r\n",
      "Hostname: testvm006\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 62660608 (59.76 MB)\r\n",
      "Non DFS Used: 8198815495 (7.64 GB)\r\n",
      "DFS Remaining: 97292673273 (90.61 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 92.17%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 7\r\n",
      "Last contact: Mon Aug 22 11:12:53 JST 2016\r\n",
      "\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!ansible {client_host} -a \"hdfs dfsadmin -report\" --become --become-user hdfs -l { target_group }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dc8c0-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dcac8-495a-11e8-8391-0242ac130002",
     "previous": "9c8dc6d6-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "デコミッションしたいノードの Name: の欄を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dcac8-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dccb2-495a-11e8-8391-0242ac130002",
     "previous": "9c8dc8c0-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "## デコミッションしたいノード情報をgroup_varsに追記する\n",
    "\n",
    "デコミッション対象のノードリストを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dccb2-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dce88-495a-11e8-8391-0242ac130002",
     "previous": "9c8dcac8-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "ansibleのユーザ変数の *datanode_decommission_nodes* に上で確認した *Name:* を記載する\n",
    "\n",
    "記載例\n",
    "\n",
    "    datanode_decommission_nodes:\n",
    "    - XXX.XXX.XXX.114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dce88-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dd536-495a-11e8-8391-0242ac130002",
     "previous": "9c8dccb2-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpbSg7h_'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "work_dir = tempfile.mkdtemp()\n",
    "work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dd536-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dd7fc-495a-11e8-8391-0242ac130002",
     "previous": "9c8dce88-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "現在の設定ファイル(`group_vars/{target_group}`)のバックアップを取っておく。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dd7fc-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dda68-495a-11e8-8391-0242ac130002",
     "previous": "9c8dd536-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [],
   "source": [
    "!cp group_vars/{target_group} {work_dir}/{target_group}-old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dda68-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8ddcac-495a-11e8-8391-0242ac130002",
     "previous": "9c8dd7fc-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "[hadoop\\_all\\_{target_group}](../tree/group_vars/) に以下のコードを追加する。(すでにDecommissionしているノードがあればリストに追加する)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8ddcac-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8ddefa-495a-11e8-8391-0242ac130002",
     "previous": "9c8dda68-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datanode_decommission_nodes:\n",
      "  - XXX.XXX.XXX.114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('datanode_decommission_nodes:\\n  - {}\\n'.format(target_host))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8ddefa-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8de1e8-495a-11e8-8391-0242ac130002",
     "previous": "9c8ddcac-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- /tmp/tmpbSg7h_/hadoop_all_testcluster-old\t2016-08-22 11:14:48.786247539 +0900\r\n",
      "+++ group_vars/hadoop_all_testcluster\t2016-08-22 11:17:48.855760027 +0900\r\n",
      "@@ -84,3 +84,5 @@\r\n",
      " swimlane_tez_version: 0.8.4\r\n",
      " swimlane_tmp_path: /tmp\r\n",
      " \r\n",
      "+datanode_decommission_nodes:\r\n",
      "+  - XXX.XXX.XXX.114\r\n",
      "\\ No newline at end of file\r\n"
     ]
    }
   ],
   "source": [
    "!diff -ur {work_dir}/{target_group}-old group_vars/{target_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8de1e8-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8de792-495a-11e8-8391-0242ac130002",
     "previous": "9c8ddefa-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "## Hadoopのコンフィグファイルを更新する\n",
    "\n",
    "上で変更した設定をHadoopのコンフィグファイルに反映させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8de792-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8de9e0-495a-11e8-8391-0242ac130002",
     "previous": "9c8de1e8-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "まず、Playbookを準備する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8de9e0-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8df0a2-495a-11e8-8391-0242ac130002",
     "previous": "9c8de792-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmpbSg7h_/hadoop'...\n",
      "remote: Counting objects: 841, done.\u001b[K\n",
      "remote: Compressing objects: 100% (292/292), done.\u001b[K\n",
      "remote: Total 841 (delta 88), reused 0 (delta 0), pack-reused 527\u001b[K\n",
      "Receiving objects: 100% (841/841), 184.13 KiB | 294.00 KiB/s, done.\n",
      "Resolving deltas: 100% (181/181), done.\n",
      "Checking connectivity... done.\n",
      "/tmp/tmpbSg7h_/hadoop\n",
      "└── playbooks\n",
      "    ├── conf_base.retry\n",
      "    ├── conf_base.yml\n",
      "    ├── conf_hdfs_base.yml\n",
      "    ├── conf_hdfs_spark.yml\n",
      "    ├── conf_hdfs_tez.yml\n",
      "    ├── conf_hdfs_yarn.yml\n",
      "    ├── conf_namenode_bootstrapstandby.yml\n",
      "    ├── conf_tez.yml\n",
      "    ├── enter_hdfs_safemode.yml\n",
      "    ├── format_namenode.yml\n",
      "    ├── group_vars\n",
      "    │   └── all\n",
      "    │       ├── base\n",
      "    │       ├── cgroups\n",
      "    │       ├── collect\n",
      "    │       ├── f500.dumpall\n",
      "    │       ├── hbase_master\n",
      "    │       ├── hbase_regionserver\n",
      "    │       ├── hcatalog\n",
      "    │       ├── hdfs_base\n",
      "    │       ├── hdfs_spark\n",
      "    │       ├── hdfs_tez\n",
      "    │       ├── hdfs_yarn\n",
      "    │       ├── hive\n",
      "    │       ├── httpfs\n",
      "    │       ├── hue\n",
      "    │       ├── java7\n",
      "    │       ├── java8\n",
      "    │       ├── journalnode\n",
      "    │       ├── mapreduce_history\n",
      "    │       ├── namenode\n",
      "    │       ├── namenode_bootstrapstandby\n",
      "    │       ├── namenode_format\n",
      "    │       ├── os\n",
      "    │       ├── pig\n",
      "    │       ├── presto_client\n",
      "    │       ├── presto_coordinator\n",
      "    │       ├── presto_user\n",
      "    │       ├── presto_worker\n",
      "    │       ├── resourcemanager\n",
      "    │       ├── site-defaults\n",
      "    │       ├── slavenode\n",
      "    │       ├── spark\n",
      "    │       ├── spark_history\n",
      "    │       ├── spark_user\n",
      "    │       ├── storm\n",
      "    │       ├── tez\n",
      "    │       └── zookeeper_server\n",
      "    ├── install-base.yml\n",
      "    ├── install_client.yml\n",
      "    ├── install_hbase_master.yml\n",
      "    ├── install_hbase_regionserver.yml\n",
      "    ├── install_hcatalog.yml\n",
      "    ├── install_hive.yml\n",
      "    ├── install_httpfs.yml\n",
      "    ├── install_hue.yml\n",
      "    ├── install_journalnode.yml\n",
      "    ├── install_mapreduce_history.yml\n",
      "    ├── install_namenode.yml\n",
      "    ├── install_pig.yml\n",
      "    ├── install_resourcemanager.yml\n",
      "    ├── install_slavenode.yml\n",
      "    ├── install_spark_historyserver.yml\n",
      "    ├── install_spark.yml\n",
      "    ├── install_timelineservice.yml\n",
      "    ├── install_zookeeper.yml\n",
      "    ├── roles\n",
      "    │   ├── base\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── conf.yml\n",
      "    │   │   │   ├── kerberos.yml\n",
      "    │   │   │   ├── keytab.yml\n",
      "    │   │   │   ├── main.yml\n",
      "    │   │   │   ├── principal.yml\n",
      "    │   │   │   └── repo.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── capacity-scheduler.xml.j2\n",
      "    │   │       ├── container-executor.cfg.j2\n",
      "    │   │       ├── core-site.xml.j2\n",
      "    │   │       ├── hadoop-env.sh.j2\n",
      "    │   │       ├── hadoop-metrics2.properties.j2\n",
      "    │   │       ├── hadoop-metrics.properties.j2\n",
      "    │   │       ├── hdfs-site.xml.j2\n",
      "    │   │       ├── hdp.repo.j2\n",
      "    │   │       ├── hosts.exclude.j2\n",
      "    │   │       ├── hosts.list.j2\n",
      "    │   │       ├── log4j.properties.j2\n",
      "    │   │       ├── mapred-env.sh.j2\n",
      "    │   │       ├── mapred-site.xml.j2\n",
      "    │   │       ├── merge-keytabs.ktutil.j2\n",
      "    │   │       ├── ssl-client.xml.j2\n",
      "    │   │       ├── ssl-server.xml.j2\n",
      "    │   │       ├── yarn-env.sh.j2\n",
      "    │   │       ├── yarn-site.xml.j2\n",
      "    │   │       └── zk-acl.txt.j2\n",
      "    │   ├── cgroups\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── conf.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   ├── main.yml\n",
      "    │   │   │   └── resource.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── cgconfig.conf.j2\n",
      "    │   │       └── cgroups.sh.j2\n",
      "    │   ├── client\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── install.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── collect\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── handlers\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── README.md\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── vars\n",
      "    │   │       └── main.yml\n",
      "    │   ├── datanode_server_deletedata\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── delete.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── f500.dumpall\n",
      "    │   │   ├── COPYING\n",
      "    │   │   ├── COPYING.LESSER\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── README.md\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       └── dumpall.j2\n",
      "    │   ├── hbase_master\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   ├── main.yml\n",
      "    │   │   │   └── principal.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── hadoop-metrics2-hbase.properties.j2\n",
      "    │   │       ├── hbase-env.sh.j2\n",
      "    │   │       ├── hbase-master.j2\n",
      "    │   │       ├── hbase-policy.xml.j2\n",
      "    │   │       ├── hbase-service-test.rb.j2\n",
      "    │   │       ├── hbase-site.xml.j2\n",
      "    │   │       ├── log4j.properties.j2\n",
      "    │   │       ├── regionservers.j2\n",
      "    │   │       └── zk-jaas.conf.j2\n",
      "    │   ├── hbase_regionserver\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── files\n",
      "    │   │   │   └── graceful_stop.sh\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   ├── main.yml\n",
      "    │   │   │   └── principal.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── hadoop-metrics2-hbase.properties.j2\n",
      "    │   │       ├── hbase-env.sh.j2\n",
      "    │   │       ├── hbase-policy.xml.j2\n",
      "    │   │       ├── hbase-regionserver.j2\n",
      "    │   │       ├── hbase-site.xml.j2\n",
      "    │   │       ├── log4j.properties.j2\n",
      "    │   │       ├── regionservers.j2\n",
      "    │   │       └── zk-jaas.conf.j2\n",
      "    │   ├── hcatalog\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       └── hcat-env.sh.j2\n",
      "    │   ├── hdfs_base\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── config.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── hdfs_spark\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── config.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── hdfs_tez\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── config.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── hdfs_yarn\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── config.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── hive\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   ├── main.yml\n",
      "    │   │   │   └── principal.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── hive-exec-log4j.properties.j2\n",
      "    │   │       ├── hive-log4j.properties.j2\n",
      "    │   │       └── hive-site.xml.j2\n",
      "    │   ├── httpfs\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── hadoop-httpfs-default.j2\n",
      "    │   │       ├── hadoop-httpfs.j2\n",
      "    │   │       ├── httpfs-env.sh.j2\n",
      "    │   │       ├── httpfs-log4j.properties.j2\n",
      "    │   │       ├── httpfs.sh.j2\n",
      "    │   │       ├── httpfs-signature.secret.j2\n",
      "    │   │       └── httpfs-site.xml.j2\n",
      "    │   ├── hue\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── hue_httpd.conf.j2\n",
      "    │   │       ├── hue.ini.j2\n",
      "    │   │       └── log.conf.j2\n",
      "    │   ├── java7\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── files\n",
      "    │   │   │   ├── env_keep_javahome\n",
      "    │   │   │   └── java.sh\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── config.yml\n",
      "    │   │       ├── install.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── java8\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── install.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── journalnode\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       └── default_hadoop-hdfs-journalnode.j2\n",
      "    │   ├── journalnode_server_createdir\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── conf.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── journalnode_server_deletedata\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── delete.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── mapreduce_history\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       └── default_hadoop-mapreduce-historyserver.j2\n",
      "    │   ├── namenode\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── default_hadoop-hdfs-namenode.j2\n",
      "    │   │       ├── default_hadoop-hdfs-zkfc.j2\n",
      "    │   │       ├── hdfs-balancer.sh.j2\n",
      "    │   │       └── jaas-hdfs.conf.j2\n",
      "    │   ├── namenode_bootstrapstandby\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── config.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── namenode_format\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── config.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── os\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── kernel.yml\n",
      "    │   │       ├── limits.yml\n",
      "    │   │       ├── main.yml\n",
      "    │   │       └── thp.yml\n",
      "    │   ├── pig\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── log4j.properties.j2\n",
      "    │   │       └── pig.properties.j2\n",
      "    │   ├── presto_client\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── config.properties.j2\n",
      "    │   │       ├── hive.properties.j2\n",
      "    │   │       ├── jvm.config.j2\n",
      "    │   │       ├── launcher.j2\n",
      "    │   │       ├── log.properties.j2\n",
      "    │   │       └── node.properties.j2\n",
      "    │   ├── presto_coordinator\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── files\n",
      "    │   │   │   └── env_keep_prestohome\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── catalog.yml\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── config.properties.j2\n",
      "    │   │       ├── hive.properties.j2\n",
      "    │   │       ├── jvm.config.j2\n",
      "    │   │       ├── launcher.j2\n",
      "    │   │       ├── log.properties.j2\n",
      "    │   │       ├── node.properties.j2\n",
      "    │   │       └── presto.sh.j2\n",
      "    │   ├── prestogres\n",
      "    │   ├── presto_user\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── main.yml\n",
      "    │   │       └── user.yml\n",
      "    │   ├── presto_worker\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── files\n",
      "    │   │   │   └── env_keep_prestohome\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── catalog.yml\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── config.properties.j2\n",
      "    │   │       ├── hive.properties.j2\n",
      "    │   │       ├── jvm.config.j2\n",
      "    │   │       ├── launcher.j2\n",
      "    │   │       ├── log.properties.j2\n",
      "    │   │       ├── node.properties.j2\n",
      "    │   │       └── presto.sh.j2\n",
      "    │   ├── resourcemanager\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       └── default_hadoop-yarn-resourcemanager.j2\n",
      "    │   ├── site-defaults\n",
      "    │   │   └── defaults\n",
      "    │   │       └── main.yml\n",
      "    │   ├── slavenode\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── default_hadoop-hdfs-datanode.j2\n",
      "    │   │       └── default_hadoop-yarn-nodemanager.j2\n",
      "    │   ├── spark\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── files\n",
      "    │   │   │   └── env_keep_sparkhome\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install-tarball.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   ├── main.yml\n",
      "    │   │   │   └── principal.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── fairscheduler.xml.j2\n",
      "    │   │       ├── log4j.properties.j2\n",
      "    │   │       ├── metrics.properties.j2\n",
      "    │   │       ├── spark-defaults.conf.j2\n",
      "    │   │       ├── spark-env.sh.j2\n",
      "    │   │       └── spark.sh.j2\n",
      "    │   ├── spark_history\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── config.yml\n",
      "    │   │       └── main.yml\n",
      "    │   ├── spark_user\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── tasks\n",
      "    │   │       ├── main.yml\n",
      "    │   │       └── user.yml\n",
      "    │   ├── storm\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── files\n",
      "    │   │   │   ├── storm-drpc\n",
      "    │   │   │   ├── storm-nimbus\n",
      "    │   │   │   ├── storm.py\n",
      "    │   │   │   ├── storm-supervisor\n",
      "    │   │   │   └── storm-ui\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   ├── main.yml\n",
      "    │   │   │   └── user.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── storm_env.ini.j2\n",
      "    │   │       ├── storm-env.sh.j2\n",
      "    │   │       ├── storm-slider-env.sh.j2\n",
      "    │   │       └── storm.yaml.j2\n",
      "    │   ├── tez\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       └── tez-site.xml.j2\n",
      "    │   ├── timelineservice\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   └── templates\n",
      "    │   │       └── default_hadoop-yarn-timelineserver.j2\n",
      "    │   ├── zookeeper_server\n",
      "    │   │   ├── defaults\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── meta\n",
      "    │   │   │   └── main.yml\n",
      "    │   │   ├── tasks\n",
      "    │   │   │   ├── config.yml\n",
      "    │   │   │   ├── install.yml\n",
      "    │   │   │   ├── main.yml\n",
      "    │   │   │   └── principal.yml\n",
      "    │   │   └── templates\n",
      "    │   │       ├── jaas.conf.j2\n",
      "    │   │       ├── log4j.properties.j2\n",
      "    │   │       ├── myid.j2\n",
      "    │   │       ├── zoo.cfg.j2\n",
      "    │   │       ├── zookeeper-env.sh.j2\n",
      "    │   │       └── zookeeper-server.j2\n",
      "    │   └── zookeeper_server_deletedata\n",
      "    │       └── tasks\n",
      "    │           ├── delete.yml\n",
      "    │           └── main.yml\n",
      "    ├── start_datanode.yml\n",
      "    ├── start_hbase_master.yml\n",
      "    ├── start_hbase_regionserver.yml\n",
      "    ├── start_hcatalog.yml\n",
      "    ├── start_httpfs.yml\n",
      "    ├── start_hue.yml\n",
      "    ├── start_journalnode.yml\n",
      "    ├── start_mapreduce_historyserver.yml\n",
      "    ├── start_namenode.retry\n",
      "    ├── start_namenode.yml\n",
      "    ├── start_nodemanager.yml\n",
      "    ├── start_resourcemanager.yml\n",
      "    ├── start_spark_historyserver.yml\n",
      "    ├── start_timelineservice.yml\n",
      "    ├── start_zookeeper-server.yml\n",
      "    ├── stop_datanode.yml\n",
      "    ├── stop_hbase_master.yml\n",
      "    ├── stop_hbase_regionserver.yml\n",
      "    ├── stop_hcatalog.yml\n",
      "    ├── stop_journalnode.yml\n",
      "    ├── stop_mapreduce_historyserver.yml\n",
      "    ├── stop_namenode.yml\n",
      "    ├── stop_nodemanager.yml\n",
      "    ├── stop_resourcemanager.yml\n",
      "    ├── stop_spark_historyserver.yml\n",
      "    ├── stop_timelineservice.yml\n",
      "    ├── stop_zookeeper-server.yml\n",
      "    ├── sync_kdc.yml\n",
      "    └── upgrade_namenode.yml\n",
      "\n",
      "194 directories, 404 files\n"
     ]
    }
   ],
   "source": [
    "!rm -fr {work_dir}/hadoop\n",
    "!git clone https://github.com/NII-cloud-operation/Literate-computing-Hadoop.git {work_dir}/hadoop\n",
    "!tree {work_dir}/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8df0a2-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8df598-495a-11e8-8391-0242ac130002",
     "previous": "9c8de9e0-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "playbook_dir = os.path.join(work_dir, 'hadoop/playbooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8df598-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8df7fa-495a-11e8-8391-0242ac130002",
     "previous": "9c8df0a2-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "Playbookを準備したら、configの更新を実施する・・・まずはdry-runしてみる。\n",
    "\n",
    "`/etc/hadoop/conf/hosts.exclude` にDecommissionしたいSlave Nodeのアドレスが追加されることを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8df7fa-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dfa5c-495a-11e8-8391-0242ac130002",
     "previous": "9c8df598-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /etc/ansible/ansible.cfg as config file\n",
      "\n",
      "PLAY [hadoop_all] **************************************************************\n",
      "\n",
      "TASK [setup] *******************************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "TASK [base : include] **********************************************************\n",
      "\u001b[0;36mincluded: /tmp/tmpbSg7h_/hadoop/playbooks/roles/base/tasks/repo.yml for XXX.XXX.XXX.70, XXX.XXX.XXX.71, XXX.XXX.XXX.72, XXX.XXX.XXX.73, XXX.XXX.XXX.112, XXX.XXX.XXX.113, XXX.XXX.XXX.114\u001b[0m\n",
      "\n",
      "TASK [base : install_hdp_repo] *************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/yum.repos.d/hdp.repo\", \"size\": 556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/yum.repos.d/hdp.repo\", \"size\": 556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/yum.repos.d/hdp.repo\", \"size\": 556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/yum.repos.d/hdp.repo\", \"size\": 556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/yum.repos.d/hdp.repo\", \"size\": 556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/yum.repos.d/hdp.repo\", \"size\": 556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/yum.repos.d/hdp.repo\", \"size\": 556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\n",
      "TASK [base : include] **********************************************************\n",
      "\u001b[0;36mincluded: /tmp/tmpbSg7h_/hadoop/playbooks/roles/base/tasks/conf.yml for XXX.XXX.XXX.70, XXX.XXX.XXX.71, XXX.XXX.XXX.72, XXX.XXX.XXX.73, XXX.XXX.XXX.112, XXX.XXX.XXX.113, XXX.XXX.XXX.114\u001b[0m\n",
      "\n",
      "TASK [base : create_hadoop_conf_dir] *******************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf\", \"size\": 4096, \"state\": \"directory\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf\", \"size\": 4096, \"state\": \"directory\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf\", \"size\": 4096, \"state\": \"directory\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf\", \"size\": 4096, \"state\": \"directory\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf\", \"size\": 4096, \"state\": \"directory\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf\", \"size\": 4096, \"state\": \"directory\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf\", \"size\": 4096, \"state\": \"directory\", \"uid\": 0}\u001b[0m\n",
      "\n",
      "TASK [base : copy_conf_files] **************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=core-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"core-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/core-site.xml\", \"size\": 2130, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=core-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"core-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/core-site.xml\", \"size\": 2130, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=core-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"core-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/core-site.xml\", \"size\": 2130, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hdfs-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hdfs-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hdfs-site.xml\", \"size\": 3556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hdfs-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hdfs-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hdfs-site.xml\", \"size\": 3556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hdfs-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hdfs-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hdfs-site.xml\", \"size\": 3556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=yarn-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-site.xml\", \"size\": 4752, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=yarn-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-site.xml\", \"size\": 4752, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=yarn-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-site.xml\", \"size\": 4752, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=mapred-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-site.xml\", \"size\": 1921, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=mapred-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-site.xml\", \"size\": 1921, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=mapred-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-site.xml\", \"size\": 1921, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hadoop-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-env.sh\", \"size\": 4623, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hadoop-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-env.sh\", \"size\": 4623, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hadoop-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-env.sh\", \"size\": 4623, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=yarn-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-env.sh\", \"size\": 4567, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=yarn-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-env.sh\", \"size\": 4567, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=yarn-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-env.sh\", \"size\": 4567, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=mapred-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-env.sh\", \"size\": 1639, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=mapred-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-env.sh\", \"size\": 1639, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=mapred-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-env.sh\", \"size\": 1639, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hadoop-metrics.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics.properties\", \"size\": 2490, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hadoop-metrics.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics.properties\", \"size\": 2490, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hadoop-metrics.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics.properties\", \"size\": 2490, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hadoop-metrics2.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics2.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics2.properties\", \"size\": 2419, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hadoop-metrics2.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics2.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics2.properties\", \"size\": 2419, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hadoop-metrics2.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics2.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics2.properties\", \"size\": 2419, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=log4j.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"log4j.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/log4j.properties\", \"size\": 11291, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=log4j.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"log4j.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/log4j.properties\", \"size\": 11291, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=log4j.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"log4j.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/log4j.properties\", \"size\": 11291, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=capacity-scheduler.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"capacity-scheduler.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/capacity-scheduler.xml\", \"size\": 4436, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=capacity-scheduler.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"capacity-scheduler.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/capacity-scheduler.xml\", \"size\": 4436, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=capacity-scheduler.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"capacity-scheduler.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/capacity-scheduler.xml\", \"size\": 4436, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.71] => (item=hosts.exclude) => {\"changed\": true, \"item\": \"hosts.exclude\"}\u001b[0m\n",
      "\u001b[0;31m--- before: /etc/hadoop/conf/hosts.exclude\n",
      "\u001b[0m\u001b[0;32m+++ after: dynamically generated\n",
      "\u001b[0m\u001b[0;36m@@ -1,2 +1,2 @@\n",
      "\u001b[0m\u001b[0;32m+XXX.XXX.XXX.114\n",
      "\u001b[0m \n",
      "\u001b[0;31m-\n",
      "\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.72] => (item=hosts.exclude) => {\"changed\": true, \"item\": \"hosts.exclude\"}\u001b[0m\n",
      "\u001b[0;31m--- before: /etc/hadoop/conf/hosts.exclude\n",
      "\u001b[0m\u001b[0;32m+++ after: dynamically generated\n",
      "\u001b[0m\u001b[0;36m@@ -1,2 +1,2 @@\n",
      "\u001b[0m\u001b[0;32m+XXX.XXX.XXX.114\n",
      "\u001b[0m \n",
      "\u001b[0;31m-\n",
      "\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.70] => (item=hosts.exclude) => {\"changed\": true, \"item\": \"hosts.exclude\"}\u001b[0m\n",
      "\u001b[0;31m--- before: /etc/hadoop/conf/hosts.exclude\n",
      "\u001b[0m\u001b[0;32m+++ after: dynamically generated\n",
      "\u001b[0m\u001b[0;36m@@ -1,2 +1,2 @@\n",
      "\u001b[0m\u001b[0;32m+XXX.XXX.XXX.114\n",
      "\u001b[0m \n",
      "\u001b[0;31m-\n",
      "\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hosts.list) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hosts.list\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hosts.list\", \"size\": 40, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hosts.list) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hosts.list\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hosts.list\", \"size\": 40, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hosts.list) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hosts.list\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hosts.list\", \"size\": 40, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=core-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"core-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/core-site.xml\", \"size\": 2130, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=core-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"core-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/core-site.xml\", \"size\": 2130, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=core-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"core-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/core-site.xml\", \"size\": 2130, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hdfs-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hdfs-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hdfs-site.xml\", \"size\": 3556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hdfs-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hdfs-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hdfs-site.xml\", \"size\": 3556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hdfs-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hdfs-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hdfs-site.xml\", \"size\": 3556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=yarn-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-site.xml\", \"size\": 4942, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=yarn-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-site.xml\", \"size\": 4942, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=yarn-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-site.xml\", \"size\": 4942, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=mapred-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-site.xml\", \"size\": 1921, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=mapred-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-site.xml\", \"size\": 1921, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=mapred-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-site.xml\", \"size\": 1921, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hadoop-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-env.sh\", \"size\": 4623, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hadoop-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-env.sh\", \"size\": 4623, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hadoop-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-env.sh\", \"size\": 4623, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=yarn-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-env.sh\", \"size\": 4567, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=yarn-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-env.sh\", \"size\": 4567, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=yarn-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-env.sh\", \"size\": 4567, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=mapred-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-env.sh\", \"size\": 1639, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=mapred-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-env.sh\", \"size\": 1639, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=mapred-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-env.sh\", \"size\": 1639, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hadoop-metrics.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics.properties\", \"size\": 2490, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hadoop-metrics.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics.properties\", \"size\": 2490, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hadoop-metrics.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics.properties\", \"size\": 2490, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hadoop-metrics2.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics2.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics2.properties\", \"size\": 2419, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hadoop-metrics2.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics2.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics2.properties\", \"size\": 2419, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hadoop-metrics2.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics2.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics2.properties\", \"size\": 2419, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=log4j.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"log4j.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/log4j.properties\", \"size\": 11291, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=log4j.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"log4j.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/log4j.properties\", \"size\": 11291, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=log4j.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"log4j.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/log4j.properties\", \"size\": 11291, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=capacity-scheduler.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"capacity-scheduler.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/capacity-scheduler.xml\", \"size\": 4436, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=capacity-scheduler.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"capacity-scheduler.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/capacity-scheduler.xml\", \"size\": 4436, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=capacity-scheduler.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"capacity-scheduler.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/capacity-scheduler.xml\", \"size\": 4436, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.112] => (item=hosts.exclude) => {\"changed\": true, \"item\": \"hosts.exclude\"}\u001b[0m\n",
      "\u001b[0;31m--- before: /etc/hadoop/conf/hosts.exclude\n",
      "\u001b[0m\u001b[0;32m+++ after: dynamically generated\n",
      "\u001b[0m\u001b[0;36m@@ -1,2 +1,2 @@\n",
      "\u001b[0m\u001b[0;32m+XXX.XXX.XXX.114\n",
      "\u001b[0m \n",
      "\u001b[0;31m-\n",
      "\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.73] => (item=hosts.exclude) => {\"changed\": true, \"item\": \"hosts.exclude\"}\u001b[0m\n",
      "\u001b[0;31m--- before: /etc/hadoop/conf/hosts.exclude\n",
      "\u001b[0m\u001b[0;32m+++ after: dynamically generated\n",
      "\u001b[0m\u001b[0;36m@@ -1,2 +1,2 @@\n",
      "\u001b[0m\u001b[0;32m+XXX.XXX.XXX.114\n",
      "\u001b[0m \n",
      "\u001b[0;31m-\n",
      "\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.113] => (item=hosts.exclude) => {\"changed\": true, \"item\": \"hosts.exclude\"}\u001b[0m\n",
      "\u001b[0;31m--- before: /etc/hadoop/conf/hosts.exclude\n",
      "\u001b[0m\u001b[0;32m+++ after: dynamically generated\n",
      "\u001b[0m\u001b[0;36m@@ -1,2 +1,2 @@\n",
      "\u001b[0m\u001b[0;32m+XXX.XXX.XXX.114\n",
      "\u001b[0m \n",
      "\u001b[0;31m-\n",
      "\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hosts.list) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hosts.list\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hosts.list\", \"size\": 40, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hosts.list) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hosts.list\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hosts.list\", \"size\": 40, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hosts.list) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hosts.list\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hosts.list\", \"size\": 40, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=core-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"core-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/core-site.xml\", \"size\": 2130, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hdfs-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hdfs-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hdfs-site.xml\", \"size\": 3556, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=yarn-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-site.xml\", \"size\": 4942, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=mapred-site.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-site.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-site.xml\", \"size\": 1921, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hadoop-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-env.sh\", \"size\": 4623, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=yarn-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"yarn-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/yarn-env.sh\", \"size\": 4567, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=mapred-env.sh) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"mapred-env.sh\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/mapred-env.sh\", \"size\": 1639, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hadoop-metrics.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics.properties\", \"size\": 2490, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hadoop-metrics2.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hadoop-metrics2.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hadoop-metrics2.properties\", \"size\": 2419, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=log4j.properties) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"log4j.properties\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/log4j.properties\", \"size\": 11291, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=capacity-scheduler.xml) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"capacity-scheduler.xml\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/capacity-scheduler.xml\", \"size\": 4436, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.114] => (item=hosts.exclude) => {\"changed\": true, \"item\": \"hosts.exclude\"}\u001b[0m\n",
      "\u001b[0;31m--- before: /etc/hadoop/conf/hosts.exclude\n",
      "\u001b[0m\u001b[0;32m+++ after: dynamically generated\n",
      "\u001b[0m\u001b[0;36m@@ -1,2 +1,2 @@\n",
      "\u001b[0m\u001b[0;32m+XXX.XXX.XXX.114\n",
      "\u001b[0m \n",
      "\u001b[0;31m-\n",
      "\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hosts.list) => {\"changed\": false, \"gid\": 0, \"group\": \"root\", \"item\": \"hosts.list\", \"mode\": \"0644\", \"owner\": \"root\", \"path\": \"/etc/hadoop/conf/hosts.list\", \"size\": 40, \"state\": \"file\", \"uid\": 0}\u001b[0m\n",
      "\n",
      "TASK [base : copy_secure_conf_files] *******************************************\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => (item=ssl-server.xml)  => {\"changed\": false, \"item\": \"ssl-server.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => (item=ssl-server.xml)  => {\"changed\": false, \"item\": \"ssl-server.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => (item=ssl-server.xml)  => {\"changed\": false, \"item\": \"ssl-server.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => (item=ssl-client.xml)  => {\"changed\": false, \"item\": \"ssl-client.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => (item=ssl-client.xml)  => {\"changed\": false, \"item\": \"ssl-client.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => (item=ssl-client.xml)  => {\"changed\": false, \"item\": \"ssl-client.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => (item=zk-acl.txt)  => {\"changed\": false, \"item\": \"zk-acl.txt\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => (item=zk-acl.txt)  => {\"changed\": false, \"item\": \"zk-acl.txt\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => (item=zk-acl.txt)  => {\"changed\": false, \"item\": \"zk-acl.txt\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => (item=container-executor.cfg)  => {\"changed\": false, \"item\": \"container-executor.cfg\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => (item=container-executor.cfg)  => {\"changed\": false, \"item\": \"container-executor.cfg\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => (item=container-executor.cfg)  => {\"changed\": false, \"item\": \"container-executor.cfg\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => (item=ssl-server.xml)  => {\"changed\": false, \"item\": \"ssl-server.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => (item=ssl-server.xml)  => {\"changed\": false, \"item\": \"ssl-server.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => (item=ssl-client.xml)  => {\"changed\": false, \"item\": \"ssl-client.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => (item=ssl-server.xml)  => {\"changed\": false, \"item\": \"ssl-server.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => (item=zk-acl.txt)  => {\"changed\": false, \"item\": \"zk-acl.txt\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => (item=ssl-client.xml)  => {\"changed\": false, \"item\": \"ssl-client.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => (item=ssl-client.xml)  => {\"changed\": false, \"item\": \"ssl-client.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => (item=container-executor.cfg)  => {\"changed\": false, \"item\": \"container-executor.cfg\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => (item=zk-acl.txt)  => {\"changed\": false, \"item\": \"zk-acl.txt\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => (item=zk-acl.txt)  => {\"changed\": false, \"item\": \"zk-acl.txt\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => (item=container-executor.cfg)  => {\"changed\": false, \"item\": \"container-executor.cfg\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => (item=container-executor.cfg)  => {\"changed\": false, \"item\": \"container-executor.cfg\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => (item=ssl-server.xml)  => {\"changed\": false, \"item\": \"ssl-server.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => (item=ssl-client.xml)  => {\"changed\": false, \"item\": \"ssl-client.xml\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => (item=zk-acl.txt)  => {\"changed\": false, \"item\": \"zk-acl.txt\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => (item=container-executor.cfg)  => {\"changed\": false, \"item\": \"container-executor.cfg\", \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\n",
      "TASK [base : include] **********************************************************\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\u001b[0m\n",
      "\n",
      "PLAY RECAP *********************************************************************\n",
      "\u001b[0;33mXXX.XXX.XXX.112\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.113\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.114\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.70\u001b[0m               : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.71\u001b[0m               : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.72\u001b[0m               : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.73\u001b[0m               : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ansible-playbook -CDv {playbook_dir}/conf_base.yml -l {target_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dfa5c-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dfc6e-495a-11e8-8391-0242ac130002",
     "previous": "9c8df7fa-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "変更内容を確認し、意図しない変更が含まれていなければ、設定変更を適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dfc6e-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8dfe44-495a-11e8-8391-0242ac130002",
     "previous": "9c8dfa5c-495a-11e8-8391-0242ac130002"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PLAY [hadoop_all] **************************************************************\n",
      "\n",
      "TASK [setup] *******************************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "TASK [base : include] **********************************************************\n",
      "\u001b[0;36mincluded: /tmp/tmpbSg7h_/hadoop/playbooks/roles/base/tasks/repo.yml for XXX.XXX.XXX.70, XXX.XXX.XXX.71, XXX.XXX.XXX.72, XXX.XXX.XXX.73, XXX.XXX.XXX.112, XXX.XXX.XXX.113, XXX.XXX.XXX.114\u001b[0m\n",
      "\n",
      "TASK [base : install_hdp_repo] *************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "TASK [base : include] **********************************************************\n",
      "\u001b[0;36mincluded: /tmp/tmpbSg7h_/hadoop/playbooks/roles/base/tasks/conf.yml for XXX.XXX.XXX.70, XXX.XXX.XXX.71, XXX.XXX.XXX.72, XXX.XXX.XXX.73, XXX.XXX.XXX.112, XXX.XXX.XXX.113, XXX.XXX.XXX.114\u001b[0m\n",
      "\n",
      "TASK [base : create_hadoop_conf_dir] *******************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112]\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "TASK [base : copy_conf_files] **************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=core-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=core-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=core-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hdfs-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hdfs-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hdfs-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=yarn-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=yarn-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=yarn-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=mapred-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=mapred-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=mapred-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hadoop-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hadoop-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hadoop-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=yarn-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=yarn-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=yarn-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=mapred-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=mapred-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=mapred-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hadoop-metrics.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hadoop-metrics.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hadoop-metrics.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hadoop-metrics2.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hadoop-metrics2.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hadoop-metrics2.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=log4j.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=log4j.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=log4j.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=capacity-scheduler.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=capacity-scheduler.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=capacity-scheduler.xml)\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.71] => (item=hosts.exclude)\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.72] => (item=hosts.exclude)\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.70] => (item=hosts.exclude)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.71] => (item=hosts.list)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.72] => (item=hosts.list)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.70] => (item=hosts.list)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=core-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=core-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=core-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hdfs-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hdfs-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hdfs-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=yarn-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=yarn-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=yarn-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=mapred-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=mapred-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=mapred-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hadoop-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hadoop-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hadoop-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=yarn-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=yarn-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=yarn-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=mapred-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=mapred-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=mapred-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hadoop-metrics.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hadoop-metrics.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hadoop-metrics.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hadoop-metrics2.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hadoop-metrics2.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hadoop-metrics2.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=log4j.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=log4j.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=log4j.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=capacity-scheduler.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=capacity-scheduler.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=capacity-scheduler.xml)\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.112] => (item=hosts.exclude)\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.113] => (item=hosts.exclude)\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.73] => (item=hosts.exclude)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.112] => (item=hosts.list)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.113] => (item=hosts.list)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.73] => (item=hosts.list)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=core-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hdfs-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=yarn-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=mapred-site.xml)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hadoop-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=yarn-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=mapred-env.sh)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hadoop-metrics.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hadoop-metrics2.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=log4j.properties)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=capacity-scheduler.xml)\u001b[0m\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.114] => (item=hosts.exclude)\u001b[0m\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114] => (item=hosts.list)\u001b[0m\n",
      "\n",
      "TASK [base : copy_secure_conf_files] *******************************************\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => (item=ssl-server.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => (item=ssl-server.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => (item=ssl-server.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => (item=ssl-client.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => (item=ssl-client.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => (item=ssl-client.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => (item=zk-acl.txt) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => (item=zk-acl.txt) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => (item=zk-acl.txt) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71] => (item=container-executor.cfg) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72] => (item=container-executor.cfg) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70] => (item=container-executor.cfg) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => (item=ssl-server.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => (item=ssl-server.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => (item=ssl-server.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => (item=ssl-client.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => (item=ssl-client.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => (item=ssl-client.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => (item=zk-acl.txt) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => (item=zk-acl.txt) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => (item=zk-acl.txt) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73] => (item=container-executor.cfg) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112] => (item=container-executor.cfg) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113] => (item=container-executor.cfg) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => (item=ssl-server.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => (item=ssl-client.xml) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => (item=zk-acl.txt) \u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114] => (item=container-executor.cfg) \u001b[0m\n",
      "\n",
      "TASK [base : include] **********************************************************\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.70]\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.71]\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.72]\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.73]\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.112]\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.113]\u001b[0m\n",
      "\u001b[0;36mskipping: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "PLAY RECAP *********************************************************************\n",
      "\u001b[0;33mXXX.XXX.XXX.112\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.113\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.114\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.70\u001b[0m               : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.71\u001b[0m               : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.72\u001b[0m               : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\u001b[0;33mXXX.XXX.XXX.73\u001b[0m               : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m6\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ansible-playbook {playbook_dir}/conf_base.yml -l {target_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8dfe44-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e001a-495a-11e8-8391-0242ac130002",
     "previous": "9c8dfc6e-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "## デコミッション設定を反映させる\n",
    "\n",
    "更新したHadoopコンフィグファイルの情報をNameNodeに反映させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e001a-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e02b8-495a-11e8-8391-0242ac130002",
     "previous": "9c8dfe44-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mXXX.XXX.XXX.70 | SUCCESS | rc=0 >>\n",
      "Refresh nodes successful\n",
      "\u001b[0m\n",
      "\u001b[0;32mXXX.XXX.XXX.71 | SUCCESS | rc=0 >>\n",
      "Refresh nodes successful\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ansible hadoop_namenode -m shell -a 'hdfs dfsadmin -fs hdfs://$(hostname):8020 -refreshNodes' -b --become-user hdfs -l {target_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e02b8-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e0592-495a-11e8-8391-0242ac130002",
     "previous": "9c8e001a-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "各ノードから *Refresh nodes successful* が表示されることを確認する\n",
    "\n",
    "このコマンドが実行されるとDataNodeのデコミッション処理が開始される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e0592-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e070e-495a-11e8-8391-0242ac130002",
     "previous": "9c8e02b8-495a-11e8-8391-0242ac130002"
    },
    "sub-structure": {
     "name": {
      "section": "デコミッションが行われたことを確認する"
     },
     "symphonic": {
      "id": "313d1f05-cf47-415e-8255-6f62ca23cc46"
     }
    }
   },
   "source": [
    "## デコミッションが行われたことを確認する\n",
    "\n",
    "Decommissionの実施状況を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e070e-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e0862-495a-11e8-8391-0242ac130002",
     "previous": "9c8e0592-495a-11e8-8391-0242ac130002"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mXXX.XXX.XXX.70 | SUCCESS | rc=0 >>\r\n",
      "Configured Capacity: 316666195968 (294.92 GB)\r\n",
      "Present Capacity: 291664450118 (271.63 GB)\r\n",
      "DFS Remaining: 291470423012 (271.45 GB)\r\n",
      "DFS Used: 194027106 (185.04 MB)\r\n",
      "DFS Used%: 0.07%\r\n",
      "Under replicated blocks: 10\r\n",
      "Blocks with corrupt replicas: 0\r\n",
      "Missing blocks: 0\r\n",
      "Missing blocks (with replication factor 1): 0\r\n",
      "\r\n",
      "-------------------------------------------------\r\n",
      "Live datanodes (4):\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.114:50010 (testvm007)\r\n",
      "Hostname: testvm007\r\n",
      "Decommission Status : Decommission in progress\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 3747840 (3.57 MB)\r\n",
      "Non DFS Used: 8199270151 (7.64 GB)\r\n",
      "DFS Remaining: 97351131385 (90.67 GB)\r\n",
      "DFS Used%: 0.00%\r\n",
      "DFS Remaining%: 92.23%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 7\r\n",
      "Last contact: Mon Aug 22 11:24:07 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.73:50010 (testvm004)\r\n",
      "Hostname: testvm004\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 63418522 (60.48 MB)\r\n",
      "Non DFS Used: 8333712922 (7.76 GB)\r\n",
      "DFS Remaining: 97157017932 (90.48 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 92.04%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 9\r\n",
      "Last contact: Mon Aug 22 11:24:08 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.112:50010 (testvm005)\r\n",
      "Hostname: testvm005\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 63446492 (60.51 MB)\r\n",
      "Non DFS Used: 8468652165 (7.89 GB)\r\n",
      "DFS Remaining: 97022050719 (90.36 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 91.92%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 11\r\n",
      "Last contact: Mon Aug 22 11:24:07 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.113:50010 (testvm006)\r\n",
      "Hostname: testvm006\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 63414252 (60.48 MB)\r\n",
      "Non DFS Used: 8199380763 (7.64 GB)\r\n",
      "DFS Remaining: 97291354361 (90.61 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 92.17%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 7\r\n",
      "Last contact: Mon Aug 22 11:24:08 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Decommissioning datanodes (1):\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.114:50010 (testvm007)\r\n",
      "Hostname: testvm007\r\n",
      "Decommission Status : Decommission in progress\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 3747840 (3.57 MB)\r\n",
      "Non DFS Used: 8199270151 (7.64 GB)\r\n",
      "DFS Remaining: 97351131385 (90.67 GB)\r\n",
      "DFS Used%: 0.00%\r\n",
      "DFS Remaining%: 92.23%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 7\r\n",
      "Last contact: Mon Aug 22 11:24:07 JST 2016\r\n",
      "\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!ansible -m shell -a \"hdfs dfsadmin -report\" --become --become-user hdfs { client_host } -l {target_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e0862-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e09a2-495a-11e8-8391-0242ac130002",
     "previous": "9c8e070e-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "デコミッション対象のDecommission Statusが *Decommissioned* になっていることを確認する\n",
    "\n",
    "+ *Decommision on progress* の場合は、処理が進行中であることを意味する。しばらくたってから再度確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e09a2-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e0ad8-495a-11e8-8391-0242ac130002",
     "previous": "9c8e0862-495a-11e8-8391-0242ac130002"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mXXX.XXX.XXX.70 | SUCCESS | rc=0 >>\r\n",
      "Configured Capacity: 316666195968 (294.92 GB)\r\n",
      "Present Capacity: 291664720454 (271.63 GB)\r\n",
      "DFS Remaining: 291470693348 (271.45 GB)\r\n",
      "DFS Used: 194027106 (185.04 MB)\r\n",
      "DFS Used%: 0.07%\r\n",
      "Under replicated blocks: 10\r\n",
      "Blocks with corrupt replicas: 0\r\n",
      "Missing blocks: 0\r\n",
      "Missing blocks (with replication factor 1): 0\r\n",
      "\r\n",
      "-------------------------------------------------\r\n",
      "Live datanodes (4):\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.114:50010 (testvm007)\r\n",
      "Hostname: testvm007\r\n",
      "Decommission Status : Decommissioned\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 3747840 (3.57 MB)\r\n",
      "Non DFS Used: 8199261959 (7.64 GB)\r\n",
      "DFS Remaining: 97351139577 (90.67 GB)\r\n",
      "DFS Used%: 0.00%\r\n",
      "DFS Remaining%: 92.23%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 7\r\n",
      "Last contact: Mon Aug 22 11:24:46 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.73:50010 (testvm004)\r\n",
      "Hostname: testvm004\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 63418522 (60.48 MB)\r\n",
      "Non DFS Used: 8333614618 (7.76 GB)\r\n",
      "DFS Remaining: 97157116236 (90.48 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 92.04%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 9\r\n",
      "Last contact: Mon Aug 22 11:24:47 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.112:50010 (testvm005)\r\n",
      "Hostname: testvm005\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 63446492 (60.51 MB)\r\n",
      "Non DFS Used: 8468578437 (7.89 GB)\r\n",
      "DFS Remaining: 97022124447 (90.36 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 91.92%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 11\r\n",
      "Last contact: Mon Aug 22 11:24:46 JST 2016\r\n",
      "\r\n",
      "\r\n",
      "Name: XXX.XXX.XXX.113:50010 (testvm006)\r\n",
      "Hostname: testvm006\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105554149376 (98.30 GB)\r\n",
      "DFS Used: 63414252 (60.48 MB)\r\n",
      "Non DFS Used: 8199282459 (7.64 GB)\r\n",
      "DFS Remaining: 97291452665 (90.61 GB)\r\n",
      "DFS Used%: 0.06%\r\n",
      "DFS Remaining%: 92.17%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 7\r\n",
      "Last contact: Mon Aug 22 11:24:47 JST 2016\r\n",
      "\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!ansible -m shell -a \"hdfs dfsadmin -report\" --sudo --sudo-user hdfs { client_host } -l {target_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e0ad8-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e0c0e-495a-11e8-8391-0242ac130002",
     "previous": "9c8e09a2-495a-11e8-8391-0242ac130002"
    },
    "sub-structure": {
     "name": {
      "section": "DataNodeを停止する"
     },
     "symphonic": {
      "id": "de2ffea4-753e-4bef-881b-f29e085feb06"
     }
    }
   },
   "source": [
    "## DataNodeを停止する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e0c0e-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e0d3a-495a-11e8-8391-0242ac130002",
     "previous": "9c8e0ad8-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "デコミッションが完了したDataNodeを停止させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e0d3a-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e0e70-495a-11e8-8391-0242ac130002",
     "previous": "9c8e0c0e-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PLAY [hadoop_slavenode] ********************************************************\n",
      "\n",
      "TASK [setup] *******************************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "TASK [stop_hadoop-hdfs-datanode] ***********************************************\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "PLAY RECAP *********************************************************************\n",
      "\u001b[0;33mXXX.XXX.XXX.114\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m2\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ansible-playbook {playbook_dir}/stop_datanode.yml -l { target_host }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e0e70-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e0fa6-495a-11e8-8391-0242ac130002",
     "previous": "9c8e0d3a-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "# NodeManagerを停止する\n",
    "\n",
    "今回はSlave Nodeとしての機能をすべて停止する・・・"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e0fa6-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e10d2-495a-11e8-8391-0242ac130002",
     "previous": "9c8e0e70-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PLAY [hadoop_slavenode] ********************************************************\n",
      "\n",
      "TASK [setup] *******************************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "TASK [stop_hadoop-yarn-nodemanager] ********************************************\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "PLAY RECAP *********************************************************************\n",
      "\u001b[0;33mXXX.XXX.XXX.114\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m2\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ansible-playbook {playbook_dir}/stop_nodemanager.yml -l { target_host }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e10d2-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e1230-495a-11e8-8391-0242ac130002",
     "previous": "9c8e0fa6-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "# RegionServerを停止する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e1230-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e13a2-495a-11e8-8391-0242ac130002",
     "previous": "9c8e10d2-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PLAY [hadoop_hbase_regionserver] ***********************************************\n",
      "\n",
      "TASK [setup] *******************************************************************\n",
      "\u001b[0;32mok: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "TASK [stop_hbase-regionserver] *************************************************\n",
      "\u001b[0;33mchanged: [XXX.XXX.XXX.114]\u001b[0m\n",
      "\n",
      "PLAY RECAP *********************************************************************\n",
      "\u001b[0;33mXXX.XXX.XXX.114\u001b[0m              : \u001b[0;32mok\u001b[0m\u001b[0;32m=\u001b[0m\u001b[0;32m2\u001b[0m    \u001b[0;33mchanged\u001b[0m\u001b[0;33m=\u001b[0m\u001b[0;33m1\u001b[0m    unreachable=0    failed=0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ansible-playbook {playbook_dir}/stop_hbase_regionserver.yml -l { target_host }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e13a2-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e14ce-495a-11e8-8391-0242ac130002",
     "previous": "9c8e1230-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "# fsckしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lc_cell_meme": {
     "current": "9c8e14ce-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e1604-495a-11e8-8391-0242ac130002",
     "previous": "9c8e13a2-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mXXX.XXX.XXX.72 | SUCCESS | rc=0 >>\r\n",
      "FSCK started by hdfs (auth:SIMPLE) from /XXX.XXX.XXX.72 for path / at Mon Aug 22 11:27:04 JST 2016\r\n",
      "..............\r\n",
      "/tmp/hadoop-yarn/staging/ansible/.staging/job_1471559811619_0001/job.jar:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073741935_1111. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      ".\r\n",
      "/tmp/hadoop-yarn/staging/ansible/.staging/job_1471559811619_0001/job.split:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073741936_1112. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      "...\r\n",
      "/tmp/hadoop-yarn/staging/ansible/.staging/job_1471559811619_0002/job.jar:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073741960_1136. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      ".\r\n",
      "/tmp/hadoop-yarn/staging/ansible/.staging/job_1471559811619_0002/job.split:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073741961_1137. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      "...\r\n",
      "/tmp/hadoop-yarn/staging/ansible/.staging/job_1471559811619_0003/job.jar:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073741990_1166. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      ".\r\n",
      "/tmp/hadoop-yarn/staging/ansible/.staging/job_1471559811619_0003/job.split:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073741991_1167. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      "...\r\n",
      "/tmp/hadoop-yarn/staging/ansible/.staging/job_1471559811619_0004/job.jar:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073742015_1191. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      ".\r\n",
      "/tmp/hadoop-yarn/staging/ansible/.staging/job_1471559811619_0004/job.split:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073742016_1192. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      "...\r\n",
      "/tmp/hadoop-yarn/staging/yarn/.staging/job_1471530986400_0001/job.jar:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073741849_1025. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      ".\r\n",
      "/tmp/hadoop-yarn/staging/yarn/.staging/job_1471530986400_0001/job.split:  Under replicated BP-975593033-XXX.XXX.XXX.70-1471530072968:blk_1073741850_1026. Target Replicas is 10 but found 3 live replica(s), 1 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      "......................Status: HEALTHY\r\n",
      " Total size:\t62628976 B (Total open files size: 664 B)\r\n",
      " Total dirs:\t71\r\n",
      " Total files:\t53\r\n",
      " Total symlinks:\t\t0 (Files currently being written: 9)\r\n",
      " Total blocks (validated):\t51 (avg. block size 1228019 B) (Total open file blocks (not validated): 8)\r\n",
      " Minimally replicated blocks:\t51 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t10 (19.607843 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t3\r\n",
      " Average block replication:\t3.7843137\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t60 (26.90583 %)\r\n",
      " DecommissionedReplicas:\t40\r\n",
      " Number of data-nodes:\t\t4\r\n",
      " Number of racks:\t\t1\r\n",
      "FSCK ended at Mon Aug 22 11:27:04 JST 2016 in 50 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/' is HEALTHYConnecting to namenode via http://testvm001:50070/fsck?ugi=hdfs&path=%2F\r\n",
      "\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!ansible hadoop_client -b -U hdfs -a 'hdfs fsck /' -l {target_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "lc_cell_meme": {
     "current": "9c8e1604-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e1730-495a-11e8-8391-0242ac130002",
     "previous": "9c8e14ce-495a-11e8-8391-0242ac130002"
    }
   },
   "source": [
    "# 後始末\n",
    "\n",
    "一時ディレクトリを削除する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "lc_cell_meme": {
     "current": "9c8e1730-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": "9c8e1870-495a-11e8-8391-0242ac130002",
     "previous": "9c8e1604-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [],
   "source": [
    "!rm -fr {work_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "lc_cell_meme": {
     "current": "9c8e1870-495a-11e8-8391-0242ac130002",
     "history": [],
     "next": null,
     "previous": "9c8e1730-495a-11e8-8391-0242ac130002"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "lc_notebook_meme": {
   "current": "9c8dad36-495a-11e8-8391-0242ac130002",
   "history": [],
   "root_cells": [
    "9c8dafa2-495a-11e8-8391-0242ac130002",
    "9c8db18c-495a-11e8-8391-0242ac130002",
    "9c8db786-495a-11e8-8391-0242ac130002",
    "9c8db9de-495a-11e8-8391-0242ac130002",
    "9c8dbd58-495a-11e8-8391-0242ac130002",
    "9c8dbf4c-495a-11e8-8391-0242ac130002",
    "9c8dc136-495a-11e8-8391-0242ac130002",
    "9c8dc32a-495a-11e8-8391-0242ac130002",
    "9c8dc514-495a-11e8-8391-0242ac130002",
    "9c8dc6d6-495a-11e8-8391-0242ac130002",
    "9c8dc8c0-495a-11e8-8391-0242ac130002",
    "9c8dcac8-495a-11e8-8391-0242ac130002",
    "9c8dccb2-495a-11e8-8391-0242ac130002",
    "9c8dce88-495a-11e8-8391-0242ac130002",
    "9c8dd536-495a-11e8-8391-0242ac130002",
    "9c8dd7fc-495a-11e8-8391-0242ac130002",
    "9c8dda68-495a-11e8-8391-0242ac130002",
    "9c8ddcac-495a-11e8-8391-0242ac130002",
    "9c8ddefa-495a-11e8-8391-0242ac130002",
    "9c8de1e8-495a-11e8-8391-0242ac130002",
    "9c8de792-495a-11e8-8391-0242ac130002",
    "9c8de9e0-495a-11e8-8391-0242ac130002",
    "9c8df0a2-495a-11e8-8391-0242ac130002",
    "9c8df598-495a-11e8-8391-0242ac130002",
    "9c8df7fa-495a-11e8-8391-0242ac130002",
    "9c8dfa5c-495a-11e8-8391-0242ac130002",
    "9c8dfc6e-495a-11e8-8391-0242ac130002",
    "9c8dfe44-495a-11e8-8391-0242ac130002",
    "9c8e001a-495a-11e8-8391-0242ac130002",
    "9c8e02b8-495a-11e8-8391-0242ac130002",
    "9c8e0592-495a-11e8-8391-0242ac130002",
    "9c8e070e-495a-11e8-8391-0242ac130002",
    "9c8e0862-495a-11e8-8391-0242ac130002",
    "9c8e09a2-495a-11e8-8391-0242ac130002",
    "9c8e0ad8-495a-11e8-8391-0242ac130002",
    "9c8e0c0e-495a-11e8-8391-0242ac130002",
    "9c8e0d3a-495a-11e8-8391-0242ac130002",
    "9c8e0e70-495a-11e8-8391-0242ac130002",
    "9c8e0fa6-495a-11e8-8391-0242ac130002",
    "9c8e10d2-495a-11e8-8391-0242ac130002",
    "9c8e1230-495a-11e8-8391-0242ac130002",
    "9c8e13a2-495a-11e8-8391-0242ac130002",
    "9c8e14ce-495a-11e8-8391-0242ac130002",
    "9c8e1604-495a-11e8-8391-0242ac130002",
    "9c8e1730-495a-11e8-8391-0242ac130002",
    "9c8e1870-495a-11e8-8391-0242ac130002"
   ]
  },
  "sub-structure": {
   "dependency": {
    "depends": [
     {
      "title": "Ansibleの設定を変える"
     },
     {
      "title": "HDFSの起動手順"
     }
    ]
   },
   "name": {
    "title": " HDFSのDataNodeのデコミッション手順"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
